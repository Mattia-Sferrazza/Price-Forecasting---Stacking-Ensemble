{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import os, gc, json, csv, itertools, logging, sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "CSV_PATH      = Path(r\"C:/Users/matti/OneDrive/Thesis/Preprocessing/dataset1.csv\")\n",
    "TARGET        = \"next_close\"\n",
    "TEST_FR       = 0.10\n",
    "N_SPLITS      = 3\n",
    "LOOKBACK_SET  = [3, 5, 8]\n",
    "EPOCHS        = 50\n",
    "PATIENCE      = 5\n",
    "BATCH_SIZE    = 32\n",
    "\n",
    "#Grid\n",
    "HP_SPACE: Dict[str, List] = {\n",
    "    \"LSTM_UNITS\":   [64, 96, 128, 192],\n",
    "    \"L2_REG\":       [1e-5, 1e-4],\n",
    "    \"DROPOUT\":      [0.05, 0.10, 0.20],\n",
    "    \"REC_DROPOUT\":  [0.05, 0.10],\n",
    "    \"LR\":           [5e-4, 1.1e-3, 2e-3],\n",
    "}\n",
    "\n",
    "#Logging\n",
    "LOG_DIR = Path(\"logs3\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "stamp   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = LOG_DIR / f\"run_{stamp}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.FileHandler(log_path), logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logging.info(\"Logging to %s\", log_path)\n",
    "\n",
    "#Eval metrics\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean(np.square(y_true - y_pred))))\n",
    "\n",
    "def mda(y_true: np.ndarray, y_pred: np.ndarray, y_prev: np.ndarray) -> float:\n",
    "    return float(np.mean(np.sign(y_pred - y_prev) == np.sign(y_true - y_prev)))\n",
    "\n",
    "# Clean data (like in GRU)\n",
    "def prepare_dataset(path: Path, lookback: int):\n",
    "    df = (pd.read_csv(path, parse_dates=[\"date\"])\n",
    "            .sort_values(\"date\").set_index(\"date\"))\n",
    "\n",
    "    df[TARGET] = df[\"close\"].shift(-1)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    feature_cols = df.columns.difference([TARGET])\n",
    "    n_test = int(len(df) * TEST_FR)\n",
    "    df_tv = df.iloc[:-n_test]\n",
    "    df_te = df.iloc[-n_test:]\n",
    "\n",
    "    def make_seq(df_part: pd.DataFrame, lb: int):\n",
    "        X_all = df_part[feature_cols].values\n",
    "        y_all = df_part[[TARGET]].values.squeeze()\n",
    "        prev = df_part[\"close\"].values\n",
    "        X_seq, y_seq, prev_seq = [], [], []\n",
    "        for t in range(lb, len(df_part)):\n",
    "            X_seq.append(X_all[t-lb:t])\n",
    "            y_seq.append(y_all[t])\n",
    "            prev_seq.append(prev[t-1])\n",
    "        return np.array(X_seq), np.array(y_seq), np.array(prev_seq)\n",
    "\n",
    "    #output unscaled\n",
    "    X_tv_raw, y_tv_raw, prev_tv = make_seq(df_tv, lookback)\n",
    "    X_te_raw, y_te_raw, prev_te = make_seq(df_te, lookback)\n",
    "\n",
    "    return X_tv_raw, y_tv_raw, prev_tv, X_te_raw, y_te_raw, prev_te\n",
    "\n",
    "\n",
    "def walk_forward_splits(n_samples: int, n_splits: int):\n",
    "    fold = n_samples // (n_splits + 1)\n",
    "    for i in range(1, n_splits + 1):\n",
    "        tr_end, va_end = fold * i, fold * (i + 1)\n",
    "        yield np.arange(tr_end), np.arange(tr_end, va_end)\n",
    "\n",
    "#architecture\n",
    "def build_model(shape: Tuple[int, int], hp: Dict) -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=shape),\n",
    "        layers.LSTM(\n",
    "            hp[\"LSTM_UNITS\"],\n",
    "            dropout=hp[\"DROPOUT\"],\n",
    "            recurrent_dropout=hp[\"REC_DROPOUT\"],\n",
    "            kernel_regularizer=regularizers.l2(hp[\"L2_REG\"]),\n",
    "            recurrent_regularizer=regularizers.l2(hp[\"L2_REG\"]),\n",
    "            bias_regularizer=regularizers.l2(hp[\"L2_REG\"]),\n",
    "        ),\n",
    "        layers.Dense(1),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp[\"LR\"], clipnorm=1.0),\n",
    "        loss=\"mse\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def grid_search(X_raw, y_raw, shape, lookback: int):\n",
    "    best_hp, best_rmse = None, np.inf\n",
    "    for values in itertools.product(*HP_SPACE.values()):\n",
    "        hp = dict(zip(HP_SPACE.keys(), values))\n",
    "        rmses = []\n",
    "\n",
    "        for tr_idx, va_idx in walk_forward_splits(len(X_raw), N_SPLITS):\n",
    "            #Fit scalers only on training fold\n",
    "            scaler_X = StandardScaler().fit(X_raw[tr_idx].reshape(-1, X_raw.shape[-1]))\n",
    "            scaler_y = StandardScaler().fit(y_raw[tr_idx].reshape(-1, 1))\n",
    "\n",
    "            #Transform training and validation sets\n",
    "            X_tr = scaler_X.transform(X_raw[tr_idx].reshape(-1, X_raw.shape[-1])).reshape(-1, lookback, X_raw.shape[-1])\n",
    "            y_tr = scaler_y.transform(y_raw[tr_idx].reshape(-1, 1)).squeeze()\n",
    "\n",
    "            X_va = scaler_X.transform(X_raw[va_idx].reshape(-1, X_raw.shape[-1])).reshape(-1, lookback, X_raw.shape[-1])\n",
    "            y_va = scaler_y.transform(y_raw[va_idx].reshape(-1, 1)).squeeze()\n",
    "\n",
    "            model = build_model(shape, hp)\n",
    "            es = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True, verbose=0)\n",
    "\n",
    "            model.fit(X_tr, y_tr,\n",
    "                      validation_data=(X_va, y_va),\n",
    "                      epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, callbacks=[es])\n",
    "\n",
    "            #Inverse-transform predictions for RMSE\n",
    "            y_pred = model.predict(X_va, verbose=0).squeeze()\n",
    "            y_pred_inv = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).squeeze()\n",
    "            y_true_inv = scaler_y.inverse_transform(y_va.reshape(-1, 1)).squeeze()\n",
    "            rmses.append(rmse(y_true_inv, y_pred_inv))\n",
    "\n",
    "            K.clear_session(); gc.collect()\n",
    "\n",
    "        avg = float(np.mean(rmses))\n",
    "        if avg < best_rmse:\n",
    "            best_rmse, best_hp = avg, hp\n",
    "\n",
    "    return best_hp, best_rmse\n",
    "\n",
    "\n",
    "#Main Run function\n",
    "def run_experiment(csv_path: Path):\n",
    "    summary_csv = LOG_DIR / \"summary2.csv\"\n",
    "    write_header = not summary_csv.exists()\n",
    "    best_overall = {}; best_rmse = np.inf; best_model = None\n",
    "\n",
    "    for lb in LOOKBACK_SET:\n",
    "        logging.info(\"=== LOOKBACK %d ===\", lb)\n",
    "\n",
    "        X_tv_raw, y_tv_raw, prev_tv, X_te_raw, y_te_raw, prev_te = prepare_dataset(csv_path, lb)\n",
    "\n",
    "        #Tune\n",
    "        hp, cv_rmse = grid_search(X_tv_raw, y_tv_raw, X_tv_raw.shape[1:], lb)\n",
    "        logging.info(\"CV RMSE %.4f with %s\", cv_rmse, hp)\n",
    "\n",
    "        #final scaler fit on full training data\n",
    "        scaler_X = StandardScaler().fit(X_tv_raw.reshape(-1, X_tv_raw.shape[-1]))\n",
    "        scaler_y = StandardScaler().fit(y_tv_raw.reshape(-1, 1))\n",
    "\n",
    "        X_tv = scaler_X.transform(X_tv_raw.reshape(-1, X_tv_raw.shape[-1])).reshape(X_tv_raw.shape)\n",
    "        y_tv = scaler_y.transform(y_tv_raw.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        X_te = scaler_X.transform(X_te_raw.reshape(-1, X_te_raw.shape[-1])).reshape(X_te_raw.shape)\n",
    "        y_te = scaler_y.transform(y_te_raw.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        #final model training\n",
    "        model = build_model(X_tv.shape[1:], hp)\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=PATIENCE,\n",
    "                                              restore_best_weights=True, verbose=0)\n",
    "        model.fit(X_tv, y_tv, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  verbose=0, callbacks=[es])\n",
    "\n",
    "        #final test evaluation\n",
    "        y_pred = model.predict(X_te, verbose=0).squeeze()\n",
    "        y_pred_inv = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).squeeze()\n",
    "        y_true_inv = scaler_y.inverse_transform(y_te.reshape(-1, 1)).squeeze()\n",
    "\n",
    "        test_rmse = rmse(y_true_inv, y_pred_inv)\n",
    "        test_mda  = mda(y_true_inv, y_pred_inv, prev_te)\n",
    "        logging.info(\"TEST - RMSE %.4f | MDA %.3f\", test_rmse, test_mda)\n",
    "\n",
    "        #save\n",
    "        with summary_csv.open(\"a\", newline=\"\") as fp:\n",
    "            w = csv.writer(fp)\n",
    "            if write_header:\n",
    "                w.writerow([\"timestamp\", \"lookback\", \"rmse\", \"mda\", *sorted(hp.keys())])\n",
    "                write_header = False\n",
    "            w.writerow([stamp, lb, test_rmse, test_mda, *[hp[k] for k in sorted(hp)]])\n",
    "\n",
    "        if test_rmse < best_rmse:\n",
    "            best_rmse, best_overall, best_model = test_rmse, dict(lookback=lb, hp=hp,\n",
    "                                                                  rmse=test_rmse, mda=test_mda), model\n",
    "\n",
    "    #save best config and model\n",
    "    best_json = LOG_DIR / f\"best_config_{stamp}.json\"\n",
    "    best_json.write_text(json.dumps(best_overall, indent=2))\n",
    "    logging.info(\"Best config saved to %s\", best_json)\n",
    "\n",
    "    MODEL_DIR = Path(\"saved_models\"); MODEL_DIR.mkdir(exist_ok=True)\n",
    "    mod_path  = MODEL_DIR / f\"lstm_best_{stamp}.keras\"\n",
    "    best_model.save(mod_path)\n",
    "    logging.info(\"Best model weights saved to %s\", mod_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(CSV_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
