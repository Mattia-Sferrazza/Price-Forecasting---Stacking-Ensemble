{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from config import *\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"YHJOJUT5YAKRNBR5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(symbols,time_from,time_to,api_key):\n",
    "    url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbols}&time_from={time_from}&topics=earnings&time_to={time_to}&limit=1000&apikey={api_key}\"\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def _get_label_sentiment(x):\n",
    "    if x <= -0.35:\n",
    "        return \"Bearish\",\"Bearish\"\n",
    "    elif -0.35 < x <= -0.15:\n",
    "        return \"Somewhat-Bearish\",\"Bearish\"\n",
    "    elif -0.15 < x < 0.15:\n",
    "        return \"Neutral\",\"Neutral\"\n",
    "    elif 0.15 <= x < 0.35:\n",
    "        return \"Somewhat_Bullish\",\"Bullish\"\n",
    "    else:  # x >= 0.35\n",
    "        return \"Bullish\",\"Bullish\"\n",
    "    \n",
    "\n",
    "_get_data(\"GOOG\",\"20200101T0000\",\"20220303T0800\",\"YHJOJUT5YAKRNBR5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Youtube video\n",
    "#https://www.youtube.com/watch?v=ascf3y7zSaY\n",
    "\n",
    "\n",
    "def get_dataset(time_from=\"20200101T0000\",\n",
    "                time_to=\"\",\n",
    "                MAX_API_CALLS_PER_DAY = 25, # Free tier only allows 25 API calls per day\n",
    "                MAX_API_CALLS_PER_MIN = 5 # Free tier only allows 5 api calls per minute\n",
    "               ):\n",
    "    data_list=[]\n",
    "    for i in range(1,MAX_API_CALLS_PER_DAY+1): \n",
    "        if i%5==0: \n",
    "            time.sleep(60)\n",
    "        \n",
    "        data=_get_data(\"GOOG\",time_from,time_to,api_key)\n",
    "        print(data)\n",
    "        if \"feed\" not in data:break\n",
    "        if len(data[\"feed\"])==0: break\n",
    "        data_list.append(data)\n",
    "        time_to=data[\"feed\"][-1][\"time_published\"][:-2] # Take all the way up to last 2 since api only takes minute level granularity\n",
    "    df=pd.concat([pd.DataFrame(data[\"feed\"]) for data in data_list])\n",
    "    # Extract TSLA specific relevance (we didn\"t use it in video)\n",
    "    df[\"ticker_relevance_GOOG\"]=df[\"ticker_sentiment\"].apply(lambda l:[el for el in l if el[\"ticker\"]==\"GOOG\"][0][\"relevance_score\"]).astype(float)\n",
    "    # Extract TSLA specific sentiment\n",
    "    df[\"ticker_sentiment_GOOG\"]=df[\"ticker_sentiment\"].apply(lambda l:[el for el in l if el[\"ticker\"]==\"GOOG\"][0][\"ticker_sentiment_score\"]).astype(float)\n",
    "    # Only take tickers with TSLA in headline \n",
    "    #df=df[df.title.str.contains(\"tsla|tesla\",case=False)]\n",
    "    # Extract # of tickers\n",
    "    df[\"num_tickers\"]=df.ticker_sentiment.apply(lambda l:len(l))\n",
    "    # Only take when # of tickers = 1\n",
    "    #df = df[df.num_tickers==1]\n",
    "    # Applying the function and creating two new columns\n",
    "    df[[\"detailed_original_label\",\"label\"]] = df.apply(lambda row: _get_label_sentiment(row[\"ticker_sentiment_GOOG\"]), axis=1, result_type=\"expand\")\n",
    "    # Drop duplicates..\n",
    "    df.drop_duplicates(subset=[\"summary\"],inplace=True,keep=\"first\")\n",
    "    # Set index to time published\n",
    "    df.set_index(\"time_published\",inplace=True)\n",
    "    # Sort by time published\n",
    "    df.sort_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(time_to=\"20220303T0800\")\n",
    "df.to_csv(\"GOOG_sentiment1000_earnings_20220303T08.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\GOOG_sentiment1000_earnings.csv\",index_col=\"time_published\").sort_index()\n",
    "earliest_time_published = df.index[0]\n",
    "earliest_time_published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = get_dataset(time_to=earliest_time_published[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\GOOG_sentiment1000_earnings.csv\", header=None)\n",
    "\n",
    "df.columns = [\n",
    "    \"timestamp\", \"title\", \"url\", \"authors\", \"summary\", \"image_url\",\n",
    "    \"source\", \"category\", \"domain\", \"topics\", \"overall_sentiment_score\",\n",
    "    \"overall_sentiment_label\", \"tickers\", \"max_relevance_score\",\n",
    "    \"max_sentiment_score\", \"ticker_count\", \"min_sentiment_label\", \"max_sentiment_label\"\n",
    "]\n",
    "\n",
    "df[\"date\"] = df[\"timestamp\"].str[:8]\n",
    "\n",
    "def extract_goog_sentiment(ticker_data):\n",
    "    try:\n",
    "        tickers = ast.literal_eval(ticker_data)\n",
    "        for ticker in tickers:\n",
    "            if ticker[\"ticker\"] == \"GOOG\":\n",
    "                return float(ticker[\"ticker_sentiment_score\"])\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "df[\"GOOG_sentiment\"] = df[\"tickers\"].apply(extract_goog_sentiment)\n",
    "\n",
    "article_counts = df.groupby(\"date\").size()\n",
    "\n",
    "goog_sentiments = df[\"GOOG_sentiment\"].dropna()\n",
    "mean_sentiment = goog_sentiments.mean()\n",
    "std_sentiment = goog_sentiments.std()\n",
    "range_sentiment = goog_sentiments.max() - goog_sentiments.min()\n",
    "max_sentiment = goog_sentiments.max()\n",
    "min_sentiment = goog_sentiments.min()\n",
    "median_sentiment = goog_sentiments.median()\n",
    "iqr_sentiment = np.percentile(goog_sentiments, 75) - np.percentile(goog_sentiments, 25)\n",
    "\n",
    "print(\"Article counts per date:\")\n",
    "print(article_counts)\n",
    "print(\"\\nGOOG Sentiment Stats:\")\n",
    "print(f\"Mean: {mean_sentiment:.4f}\")\n",
    "print(f\"Standard Deviation: {std_sentiment:.4f}\")\n",
    "print(f\"IQR: {iqr_sentiment:.4f}\")\n",
    "print(f\"Median: {median_sentiment:.4f}\")\n",
    "print(f\"Range: {range_sentiment:.4f}\")\n",
    "print(f\"Max: {max_sentiment:.4f}\")\n",
    "print(f\"Min: {min_sentiment:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\GOOG_sentiment1000_earnings.csv\"\n",
    "file2 = r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\GOOG_sentiment1000_earnings_20230213T2040.csv\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "#Combine with df1 so that df2\"s oldest entries are first\n",
    "combined_df = pd.concat([df2, df1], ignore_index=True)\n",
    "\n",
    "\n",
    "#Save to a new file\n",
    "combined_df.to_csv(\"aggregated_news_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\aggregated_news_data.csv\")\n",
    "#print(combined_df.columns)\n",
    "\n",
    "final = combined_df[[\"time_published\", \"title\", \"summary\", \"ticker_relevance_GOOG\", \"ticker_sentiment_GOOG\"]]\n",
    "\n",
    "final.to_csv(\"filtered_sentiment_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\filtered_sentiment_data.csv\")\n",
    "\n",
    "df[\"time_published\"] = pd.to_datetime(df[\"time_published\"], format=\"%Y%m%dT%H%M%S\")\n",
    "df[\"time_published\"] = df[\"time_published\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "duplicates_title = df[df.duplicated(subset=[\"title\",\"time_published\"], keep=False)]\n",
    "print(len(duplicates_title))\n",
    "\n",
    "print(duplicates_title[[\"time_published\", \"title\"]].sort_values(\"title\"))\n",
    "print()\n",
    "\n",
    "#Checked a few and seems like the titles are re-used, but the articles are different\n",
    "#Authors are usually the same, with exception of a few, but topics, relevance scores and sentiment scores are different\n",
    "duplicates = df[df.duplicated(subset=\"title\", keep=False)]\n",
    "print(len(duplicates))\n",
    "\n",
    "print(duplicates[[\"time_published\", \"title\"]].sort_values(\"title\"))\n",
    "\n",
    "\n",
    "df_cleaned = df.drop_duplicates(subset=[\"title\", \"time_published\"], keep=\"first\")\n",
    "df_cleaned.to_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\filtered_sentiment_data_no_duplicates.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\filtered_sentiment_data_no_duplicates.csv\")\n",
    "\n",
    "#Calculate weighted sum\n",
    "df[\"time_published\"] = pd.to_datetime(df[\"time_published\"])\n",
    "df[\"date\"] = df[\"time_published\"].dt.date\n",
    "\n",
    "daily_sentiment = (\n",
    "    df.groupby(\"date\")\n",
    "    .apply(lambda g: (g[\"ticker_sentiment_GOOG\"] * g[\"ticker_relevance_GOOG\"]).sum() / g[\"ticker_relevance_GOOG\"].sum())\n",
    "    .reset_index(name=\"daily_weighted_sentiment_GOOG\"))\n",
    "\n",
    "daily_sentiment.to_csv(\"F_daily_sentiment_GOOG.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing dates\n",
    "df = pd.read_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\F_daily_sentiment_GOOG.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "df.set_index(\"date\", inplace=True)\n",
    "\n",
    "start_date = \"2022-03-03\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "full_range = pd.date_range(start=start_date, end=end_date)\n",
    "missing_dates = full_range.difference(df.index)\n",
    "print(missing_dates)\n",
    "\n",
    "#Missing 2022-03-27 since the very start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix missing date with forward filling\n",
    "\n",
    "trading_idx = pd.date_range(start=start_date, end=end_date, freq='B')  #'B' = business days\n",
    "fwd_1d = df.reindex(trading_idx, method=\"ffill\", limit=3)\n",
    "added_rows = fwd_1d.index.difference(df.index)\n",
    "\n",
    "common_idx = df.index.intersection(fwd_1d.index)\n",
    "original_unchanged = fwd_1d.loc[common_idx].equals(df.loc[common_idx])\n",
    "print(\"Original data unchanged:\", original_unchanged)\n",
    "\n",
    "fwd_1d.index.name = \"date\"\n",
    "\n",
    "fwd_1d.to_csv(\"FF_daily_sentiment_GOOG.csv\", index=True)\n",
    "\n",
    "#Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folling sentiment features, as it could be relevant to know not just yesterday's sentimement but further past\n",
    "df = pd.read_csv(r\"C:\\Users\\matti\\OneDrive\\Thesis\\Data\\FF_daily_sentiment_GOOG.csv\", parse_dates=[\"date\"])\n",
    "df.set_index(\"date\", inplace=True)\n",
    "\n",
    "#Merge with rest of data\n",
    "sent = df[\"daily_weighted_sentiment_GOOG\"]\n",
    "\n",
    "roll_3d = sent.rolling(3).mean().reindex(trading_idx, method=\"ffill\", limit=3).to_frame(name=\"sent_roll_3d\")\n",
    "roll_5d = sent.rolling(5).mean().reindex(trading_idx, method=\"ffill\", limit=5).to_frame(name=\"sent_roll_5d\")\n",
    "roll_7d = sent.rolling(7).mean().reindex(trading_idx, method=\"ffill\", limit=7).to_frame(name=\"sent_roll_7d\")\n",
    "\n",
    "sent_features = pd.concat([fwd_1d, roll_3d, roll_5d, roll_7d], axis=1)\n",
    "\n",
    "#Merge with price and indicator data\n",
    "sent_features.to_csv(\"FINAL_daily_sentiment_GOOG.csv\", index=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
